{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "720e4280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math #important library , calculate log\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bd950e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample collection of documents\n",
    "documents = [\n",
    "    \"Scientists have discovered a new species of marine life in the deep ocean.\",\n",
    "    \"NASA's Mars rover is searching for signs of ancient life on the Red Planet.\",\n",
    "    \"The stock market experienced a significant drop in trading today.\",\n",
    "    \"Astronomers have identified a distant galaxy with unusual star formations.\",\n",
    "    \"The government announced new measures to combat climate change.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5ea50c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for lemmatization (a simple example, not comprehensive)\n",
    "lemmatization_dict = {\n",
    "    \"species\": \"specie\",\n",
    "    \"species\": \"species\",\n",
    "    \"oceans\": \"ocean\",\n",
    "    \"ocean's\": \"ocean\",\n",
    "    \"rover\": \"rover\",\n",
    "    \"discovered\":\"discover\",\n",
    "    \"experienced\":\"experience\",\n",
    "    \"rovers\": \"rover\",\n",
    "    \"trading\": \"trade\",\n",
    "    \"identified\": \"identify\",\n",
    "    \"identifies\": \"identify\",\n",
    "    \"formations\": \"formation\",\n",
    "    \"governments\": \"government\",\n",
    "    \"measures\": \"measure\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16f1f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# terms = [lemmatization_dict.get(term, term) for term in terms]\n",
    "# terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed247b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['scientists',\n",
       "  'have',\n",
       "  'discover',\n",
       "  'a',\n",
       "  'new',\n",
       "  'species',\n",
       "  'of',\n",
       "  'marine',\n",
       "  'life',\n",
       "  'in',\n",
       "  'the',\n",
       "  'deep',\n",
       "  'ocean'],\n",
       " [\"nasa's\",\n",
       "  'mars',\n",
       "  'rover',\n",
       "  'is',\n",
       "  'searching',\n",
       "  'for',\n",
       "  'signs',\n",
       "  'of',\n",
       "  'ancient',\n",
       "  'life',\n",
       "  'on',\n",
       "  'the',\n",
       "  'red',\n",
       "  'planet'],\n",
       " ['the',\n",
       "  'stock',\n",
       "  'market',\n",
       "  'experience',\n",
       "  'a',\n",
       "  'significant',\n",
       "  'drop',\n",
       "  'in',\n",
       "  'trade',\n",
       "  'today'],\n",
       " ['astronomers',\n",
       "  'have',\n",
       "  'identify',\n",
       "  'a',\n",
       "  'distant',\n",
       "  'galaxy',\n",
       "  'with',\n",
       "  'unusual',\n",
       "  'star',\n",
       "  'formation'],\n",
       " ['the',\n",
       "  'government',\n",
       "  'announced',\n",
       "  'new',\n",
       "  'measure',\n",
       "  'to',\n",
       "  'combat',\n",
       "  'climate',\n",
       "  'change']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize documents into words (terms), remove punctuation, and lemmatize\n",
    "def preprocess_text(document):\n",
    "    terms = document.lower().split()\n",
    "    terms = [term.strip(string.punctuation) for term in terms]\n",
    "    terms = [lemmatization_dict.get(term, term) for term in terms]\n",
    "    return terms\n",
    "list_docs = [preprocess_text(document) for document in documents]\n",
    "list_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34cee75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'ancient',\n",
       " 'announced',\n",
       " 'astronomers',\n",
       " 'change',\n",
       " 'climate',\n",
       " 'combat',\n",
       " 'deep',\n",
       " 'discover',\n",
       " 'distant',\n",
       " 'drop',\n",
       " 'experience',\n",
       " 'for',\n",
       " 'formation',\n",
       " 'galaxy',\n",
       " 'government',\n",
       " 'have',\n",
       " 'identify',\n",
       " 'in',\n",
       " 'is',\n",
       " 'life',\n",
       " 'marine',\n",
       " 'market',\n",
       " 'mars',\n",
       " 'measure',\n",
       " \"nasa's\",\n",
       " 'new',\n",
       " 'ocean',\n",
       " 'of',\n",
       " 'on',\n",
       " 'planet',\n",
       " 'red',\n",
       " 'rover',\n",
       " 'scientists',\n",
       " 'searching',\n",
       " 'significant',\n",
       " 'signs',\n",
       " 'species',\n",
       " 'star',\n",
       " 'stock',\n",
       " 'the',\n",
       " 'to',\n",
       " 'today',\n",
       " 'trade',\n",
       " 'unusual',\n",
       " 'with'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a set of unique terms (vocabulary)\n",
    "vocabulary = set(y for x in list_docs for y in x)\n",
    "\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da0c442a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': [0, 0, 0, 0, 0],\n",
       " 'searching': [0, 0, 0, 0, 0],\n",
       " 'experience': [0, 0, 0, 0, 0],\n",
       " 'is': [0, 0, 0, 0, 0],\n",
       " 'of': [0, 0, 0, 0, 0],\n",
       " 'today': [0, 0, 0, 0, 0],\n",
       " 'discover': [0, 0, 0, 0, 0],\n",
       " 'in': [0, 0, 0, 0, 0],\n",
       " 'trade': [0, 0, 0, 0, 0],\n",
       " 'drop': [0, 0, 0, 0, 0],\n",
       " 'for': [0, 0, 0, 0, 0],\n",
       " 'significant': [0, 0, 0, 0, 0],\n",
       " 'the': [0, 0, 0, 0, 0],\n",
       " 'red': [0, 0, 0, 0, 0],\n",
       " 'signs': [0, 0, 0, 0, 0],\n",
       " 'species': [0, 0, 0, 0, 0],\n",
       " 'deep': [0, 0, 0, 0, 0],\n",
       " 'change': [0, 0, 0, 0, 0],\n",
       " 'ocean': [0, 0, 0, 0, 0],\n",
       " 'unusual': [0, 0, 0, 0, 0],\n",
       " 'climate': [0, 0, 0, 0, 0],\n",
       " 'distant': [0, 0, 0, 0, 0],\n",
       " 'combat': [0, 0, 0, 0, 0],\n",
       " 'mars': [0, 0, 0, 0, 0],\n",
       " 'rover': [0, 0, 0, 0, 0],\n",
       " 'scientists': [0, 0, 0, 0, 0],\n",
       " 'ancient': [0, 0, 0, 0, 0],\n",
       " 'astronomers': [0, 0, 0, 0, 0],\n",
       " 'government': [0, 0, 0, 0, 0],\n",
       " \"nasa's\": [0, 0, 0, 0, 0],\n",
       " 'marine': [0, 0, 0, 0, 0],\n",
       " 'announced': [0, 0, 0, 0, 0],\n",
       " 'market': [0, 0, 0, 0, 0],\n",
       " 'galaxy': [0, 0, 0, 0, 0],\n",
       " 'formation': [0, 0, 0, 0, 0],\n",
       " 'planet': [0, 0, 0, 0, 0],\n",
       " 'with': [0, 0, 0, 0, 0],\n",
       " 'new': [0, 0, 0, 0, 0],\n",
       " 'on': [0, 0, 0, 0, 0],\n",
       " 'to': [0, 0, 0, 0, 0],\n",
       " 'stock': [0, 0, 0, 0, 0],\n",
       " 'identify': [0, 0, 0, 0, 0],\n",
       " 'have': [0, 0, 0, 0, 0],\n",
       " 'life': [0, 0, 0, 0, 0],\n",
       " 'star': [0, 0, 0, 0, 0],\n",
       " 'measure': [0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary to store the term frequency (TF) for each term in each document\n",
    "tf_values = {term: [0] * len(documents) for term in vocabulary}\n",
    "tf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4fc32da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Term Frequency (TF)\n",
    "for i, document in enumerate(documents):\n",
    "    terms = preprocess_text(document)\n",
    "    for term in terms:\n",
    "        tf_values[term][i] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11d91fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a : 0.22314355131420976\n",
      "searching : 0.9162907318741551\n",
      "experience : 0.9162907318741551\n",
      "is : 0.9162907318741551\n",
      "of : 0.5108256237659907\n",
      "today : 0.9162907318741551\n",
      "discover : 0.9162907318741551\n",
      "in : 0.5108256237659907\n",
      "trade : 0.9162907318741551\n",
      "drop : 0.9162907318741551\n",
      "for : 0.9162907318741551\n",
      "significant : 0.9162907318741551\n",
      "the : 0.0\n",
      "red : 0.9162907318741551\n",
      "signs : 0.9162907318741551\n",
      "species : 0.9162907318741551\n",
      "deep : 0.9162907318741551\n",
      "change : 0.9162907318741551\n",
      "ocean : 0.9162907318741551\n",
      "unusual : 0.9162907318741551\n",
      "climate : 0.9162907318741551\n",
      "distant : 0.9162907318741551\n",
      "combat : 0.9162907318741551\n",
      "mars : 0.9162907318741551\n",
      "rover : 0.9162907318741551\n",
      "scientists : 0.9162907318741551\n",
      "ancient : 0.9162907318741551\n",
      "astronomers : 0.9162907318741551\n",
      "government : 0.9162907318741551\n",
      "nasa's : 0.9162907318741551\n",
      "marine : 0.9162907318741551\n",
      "announced : 0.9162907318741551\n",
      "market : 0.9162907318741551\n",
      "galaxy : 0.9162907318741551\n",
      "formation : 0.9162907318741551\n",
      "planet : 0.9162907318741551\n",
      "with : 0.9162907318741551\n",
      "new : 0.5108256237659907\n",
      "on : 0.9162907318741551\n",
      "to : 0.9162907318741551\n",
      "stock : 0.9162907318741551\n",
      "identify : 0.9162907318741551\n",
      "have : 0.5108256237659907\n",
      "life : 0.5108256237659907\n",
      "star : 0.9162907318741551\n",
      "measure : 0.9162907318741551\n"
     ]
    }
   ],
   "source": [
    "# Calculate Inverse Document Frequency (IDF)\n",
    "idf_values = {}\n",
    "total_documents = len(documents)\n",
    "for term in vocabulary:\n",
    "    document_occurences = sum([1 for document in documents if term in preprocess_text(document)])\n",
    "    idf_values[term] = math.log(total_documents / (document_occurences+1))\n",
    "    print(term,\":\",idf_values[term])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be824663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.22314355131420976,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.5108256237659907,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.5108256237659907,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.5108256237659907,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.5108256237659907,\n",
       "  0.5108256237659907,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.5108256237659907,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.5108256237659907,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.22314355131420976,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.5108256237659907,\n",
       "  0.9162907318741551,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.22314355131420976,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.5108256237659907,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.5108256237659907,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate TF-IDF values\n",
    "tfidf_values = []\n",
    "for i, document in enumerate(documents):\n",
    "    terms = preprocess_text(document)\n",
    "    tfidf_document = []\n",
    "    for term in vocabulary:\n",
    "        tf = tf_values[term][i]\n",
    "        idf = idf_values[term]\n",
    "        tfidf = tf * idf\n",
    "        tfidf_document.append(tfidf)\n",
    "    tfidf_values.append(tfidf_document)\n",
    "tfidf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99f53d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF:\n",
      "          a  searching  experience        is        of     today  discover  \\\n",
      "0  0.223144   0.000000    0.000000  0.000000  0.510826  0.000000  0.916291   \n",
      "1  0.000000   0.916291    0.000000  0.916291  0.510826  0.000000  0.000000   \n",
      "2  0.223144   0.000000    0.916291  0.000000  0.000000  0.916291  0.000000   \n",
      "3  0.223144   0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "4  0.000000   0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "         in     trade      drop  ...      with       new        on        to  \\\n",
      "0  0.510826  0.000000  0.000000  ...  0.000000  0.510826  0.000000  0.000000   \n",
      "1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.916291  0.000000   \n",
      "2  0.510826  0.916291  0.916291  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "3  0.000000  0.000000  0.000000  ...  0.916291  0.000000  0.000000  0.000000   \n",
      "4  0.000000  0.000000  0.000000  ...  0.000000  0.510826  0.000000  0.916291   \n",
      "\n",
      "      stock  identify      have      life      star   measure  \n",
      "0  0.000000  0.000000  0.510826  0.510826  0.000000  0.000000  \n",
      "1  0.000000  0.000000  0.000000  0.510826  0.000000  0.000000  \n",
      "2  0.916291  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "3  0.000000  0.916291  0.510826  0.000000  0.916291  0.000000  \n",
      "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.916291  \n",
      "\n",
      "[5 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert TF-IDF values to a DataFrame\n",
    "df_tfidf = pd.DataFrame(tfidf_values, columns=list(vocabulary))\n",
    "\n",
    "# Display TF-IDF results\n",
    "print(\"TF-IDF:\")\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9fcbe8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save TF-IDF results to a CSV file (optional)\n",
    "# df_tfidf.to_csv(\"tfidf_custom_preprocessed_news.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36077c4e",
   "metadata": {},
   "source": [
    "# Using Libraries for Lemmatization and Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0495e332",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# if your machine doesn't have these libraries, you need to install them\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# if your machine doesn't have these libraries, you need to install them\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# download the punkt\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69087c67",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize NLTK's lemmatizer and download stopwords\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize NLTK's lemmatizer\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize NLTK's lemmatizer and download stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "# Initialize NLTK's lemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Tokenize documents into words (terms), remove punctuation, lemmatize, and remove stopwords\n",
    "def preprocess_text(document):\n",
    "    terms = nltk.word_tokenize(document)\n",
    "    terms = [term.strip(string.punctuation) for term in terms]\n",
    "    terms = [ps.stem(term) for term in terms]\n",
    "    terms = [term.lower() for term in terms if term not in stopwords.words('english')]\n",
    "    return ' '.join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c050db8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessed_documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Preprocess the text in the documents\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m preprocessed_documents \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessed_documents\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create a TfidfVectorizer instance\u001b[39;00m\n\u001b[0;32m      5\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessed_documents' is not defined"
     ]
    }
   ],
   "source": [
    "# Preprocess the text in the documents\n",
    "preprocessed_documents = preprocessed_documents\n",
    "\n",
    "# Create a TfidfVectorizer instance\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ded965f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fit and transform the preprocessed documents to compute TF-IDF values CADT@0zJanZ!\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241m.\u001b[39mfit_transform(preprocessed_documents)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Convert the TF-IDF matrix to a DataFrame\u001b[39;00m\n\u001b[0;32m      5\u001b[0m df_tfidf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(tfidf_matrix\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Fit and transform the preprocessed documents to compute TF-IDF values CADT@0zJanZ!\n",
    "tfidf_matrix = vectorizer.fit_transform(preprocessed_documents)\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2e4c443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Display TF-IDF results\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF-IDF:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf_tfidf\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "# Display TF-IDF results\n",
    "print(\"TF-IDF:\")\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc40a2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f807798e-e8bb-4964-bb82-28f68ff487fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
